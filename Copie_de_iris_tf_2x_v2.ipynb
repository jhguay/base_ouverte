{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhguay/base_ouverte/blob/main/Copie_de_iris_tf_2x_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UJ0pipE1lIP"
      },
      "source": [
        "[![](http://bec552ebfe.url-de-test.ws/ml/buttonBackProp.png)](https://www.backprop.fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUCMEhba3yXj"
      },
      "source": [
        "# <font color=\"teal\">Introduction</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jElLULrDhQZR"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfV2Dai0Ow2o"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Wzg69bnwK2"
      },
      "source": [
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6c7uEU9rjRM"
      },
      "source": [
        "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
        "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
        "                                           origin=train_dataset_url)\n",
        "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXkNu0XxGSck"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "print (iris['DESCR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9LYVsljIn4s"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8O58XwTJiVn"
      },
      "source": [
        "iris_data = pd.DataFrame(data=iris['data'],columns=iris['feature_names'])\n",
        "iris_data[\"target\"] = iris['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1zypDbeJpxR"
      },
      "source": [
        "color_wheel = {\n",
        "    1: \"#0392cf\",\n",
        "    2: \"#7bc043\",\n",
        "    3: \"#ee4035\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT_k9egGJyJg"
      },
      "source": [
        "colors = iris_data[\"target\"].map(lambda x: color_wheel.get(x + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQvb_JYdrpPm"
      },
      "source": [
        "!head -n5 {train_dataset_fp}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQhzD6P-uBoq"
      },
      "source": [
        "From this view of the dataset, notice the following:\n",
        "\n",
        "1. The first line is a header containing information about the dataset:\n",
        "  * There are 120 total examples. Each example has four features and one of three possible label names.\n",
        "2. Subsequent rows are data records, one *[example](https://developers.google.com/machine-learning/glossary/#example)* per line, where:\n",
        "  * The first four fields are *[features](https://developers.google.com/machine-learning/glossary/#feature)*: these are the characteristics of an example. Here, the fields hold float numbers representing flower measurements.\n",
        "  * The last column is the *[label](https://developers.google.com/machine-learning/glossary/#label)*: this is the value we want to predict. For this dataset, it's an integer value of 0, 1, or 2 that corresponds to a flower name.\n",
        "\n",
        "Let's write that out in code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Edhevw7exl6"
      },
      "source": [
        "# column order in CSV file\n",
        "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
        "\n",
        "feature_names = column_names[:-1]\n",
        "label_name = column_names[-1]\n",
        "\n",
        "print(\"Features: {}\".format(feature_names))\n",
        "print(\"Label: {}\".format(label_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCtwLoJhhDNc"
      },
      "source": [
        "Each label is associated with string name (for example, \"setosa\"), but machine learning typically relies on numeric values. The label numbers are mapped to a named representation, such as:\n",
        "\n",
        "* `0`: Iris setosa\n",
        "* `1`: Iris versicolor\n",
        "* `2`: Iris virginica\n",
        "\n",
        "For more information about features and labels, see the [ML Terminology section of the Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNlJlUOhkoX"
      },
      "source": [
        "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqPkQExM2Pwt"
      },
      "source": [
        "# Create a `tf.data.Dataset`\n",
        "\n",
        "TensorFlow's [Dataset API](../../guide/data.ipynb) handles many common cases for loading data into a model. This is a high-level API for reading data and transforming it into a form used for training.\n",
        "\n",
        "\n",
        "Since the dataset is a CSV-formatted text file, use the `tf.data.experimental.make_csv_dataset` function to parse the data into a suitable format. Since this function generates data for training models, the default behavior is to shuffle the data (`shuffle=True, shuffle_buffer_size=10000`), and repeat the dataset forever (`num_epochs=None`). We also set the [batch_size](https://developers.google.com/machine-learning/glossary/#batch_size) parameter:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w3oOYLYO_sH"
      },
      "source": [
        "[![](https://raw.githubusercontent.com/BackProp-fr/meetup/master/images/LogoBackPropTranspSmall.png)](https://www.backprop.fr)\n",
        "\n",
        "<font color=\"LightSlateGray\">On lira avec profit l'article [Dataset 5 fleurs](http://tensorflow.backprop.fr/dataset-5-fleurs/) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsxHnz1ebJ2S"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = tf.data.experimental.make_csv_dataset(\n",
        "    train_dataset_fp,\n",
        "    batch_size,\n",
        "    column_names=column_names,\n",
        "    label_name=label_name,\n",
        "    num_epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size"
      ],
      "metadata": {
        "id": "v82gjPTGdGYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB_RSn62c-3G"
      },
      "source": [
        "The `make_csv_dataset` function returns a `tf.data.Dataset` of `(features, label)` pairs, where `features` is a dictionary: `{'feature_name': value}`\n",
        "\n",
        "These `Dataset` objects are iterable. Let's look at a batch of features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDuG94H-C122"
      },
      "source": [
        "features, labels = next(iter(train_dataset))\n",
        "print(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maJD3cgwbpeJ"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNfoNwCLHsrn"
      },
      "source": [
        "[![](https://raw.githubusercontent.com/BackProp-fr/meetup/master/images/LogoBackPropTranspSmall.png)](https://www.backprop.fr)\n",
        "\n",
        "<font color=\"LightSlateGray\">Une autre fa√ßon de voir un \"batch of features\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOAEOzOZ9I1J"
      },
      "source": [
        "takeone = train_dataset.take(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtPpEO9e-ZGa"
      },
      "source": [
        "for data in takeone:\n",
        "  print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwEX6d_0AyMn"
      },
      "source": [
        "data[0]['sepal_length']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E63mArnQaAGz"
      },
      "source": [
        "Notice that like-features are grouped together, or *batched*. Each example row's fields are appended to the corresponding feature array. Change the `batch_size` to set the number of examples stored in these feature arrays.\n",
        "\n",
        "You can start to see some clusters by plotting a few features from the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me5Wn-9FcyyO"
      },
      "source": [
        "plt.scatter(features['petal_length'],\n",
        "            features['sepal_length'],\n",
        "            c=labels,\n",
        "            cmap='viridis')\n",
        "\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Sepal length\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlxpSyHlhT6M"
      },
      "source": [
        "To simplify the model building step, create a function to repackage the features dictionary into a single array with shape: `(batch_size, num_features)`.\n",
        "\n",
        "This function uses the `tf.stack` method which takes values from a list of tensors and creates a combined tensor at the specified dimension:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzSJZfASTfPf"
      },
      "source": [
        "## <font color=\"orange\">tf.stack</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naoAfPuGIvAV"
      },
      "source": [
        "[![](https://raw.githubusercontent.com/BackProp-fr/meetup/master/images/LogoBackPropTranspSmall.png)](https://www.backprop.fr)\n",
        "\n",
        "<font color=\"LightSlateGray\">tf.stack stacks a list of rank-R tensors into one rank-(R+1) tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgQwD233KRW7"
      },
      "source": [
        "x = tf.constant([1, 4]) \n",
        "y = tf.constant([2, 5]) \n",
        "z = tf.constant([3, 6]) \n",
        "tf.stack([x, y, z]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIFB3Al9LoXY"
      },
      "source": [
        "[![](https://raw.githubusercontent.com/BackProp-fr/meetup/master/images/LogoBackPropTranspSmall.png)](https://www.backprop.fr)\n",
        "\n",
        "<font color=\"LightSlateGray\">Given a list of length N of tensors of shape (A, B, C);\n",
        "\n",
        "<font color=\"LightSlateGray\">if axis == 0 then the output tensor will have the shape (N, A, B, C). if axis == 1 then the output tensor will have the shape (A, N, B, C)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTHAYjoKKonu"
      },
      "source": [
        "tf.stack([x, y, z], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbDkzGZIkpXf"
      },
      "source": [
        "train_dataset = train_dataset.map(pack_features_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLy0Q1xCldVO"
      },
      "source": [
        "The features element of the `Dataset` are now arrays with shape `(batch_size, num_features)`. Let's look at the first few examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kex9ibEek6Tr"
      },
      "source": [
        "features, labels = next(iter(train_dataset))\n",
        "print(features[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SKHXgg9MK2Y"
      },
      "source": [
        "print (labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGSkA7pRM0Pl"
      },
      "source": [
        "takeone = train_dataset.take(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbDOwIWnM4Zq"
      },
      "source": [
        "for data in takeone:\n",
        "  print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sa0OPLHNN59"
      },
      "source": [
        "data[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2UbdWZoNWnb"
      },
      "source": [
        "data[1][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWzfg8vfXMxk"
      },
      "source": [
        "### <font color=\"green\">fa√ßon de faire habituelle</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn0S7FaOYwDI"
      },
      "source": [
        "[![](https://raw.githubusercontent.com/BackProp-fr/meetup/master/images/LogoBackPropTranspSmall.png)](https://www.backprop.fr)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRR_L-OyHC-k"
      },
      "source": [
        "model_2 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcASI3YCYRer"
      },
      "source": [
        "model_2.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIcVmbrCXles"
      },
      "source": [
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz8tgeK5NTzN"
      },
      "source": [
        "%%time\n",
        "history = model_2.fit(train_dataset, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wFKnhWCpDSS"
      },
      "source": [
        "### Using the model\n",
        "\n",
        "Let's have a quick look at what this model does to a batch of features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe6SQ5NrpB-I"
      },
      "source": [
        "predictions = model_2(features)\n",
        "predictions[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "micUCJG7ZouC"
      },
      "source": [
        "[![](https://raw.githubusercontent.com/BackProp-fr/meetup/master/images/LogoBackPropTranspSmall.png)](https://www.backprop.fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSopEiO0ZwlO"
      },
      "source": [
        "<font color=\"LightSlateGray\">logit, softmax et argmax permettent d'expliquer ce qui est fait par le r√©seau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDajsH7BdslK"
      },
      "source": [
        "<font color=\"LightSlateGray\">La derni√®re couche du r√©seau de neurones, est la couche logit. C'est la sortie du r√©seau. Si comme ici on a 3 classes alors on aura 3 neurones, 3 valeurs en output. On applique ensuite un softmax pour avoir des probabilit√©s puis on prend la valeur la plus grande pour avoir notre classe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxyXOhwVr5S3"
      },
      "source": [
        "Here, each example returns a [logit](https://developers.google.com/machine-learning/crash-course/glossary#logits) for each class.\n",
        "\n",
        "To convert these logits to a probability for each class, use the [softmax](https://developers.google.com/machine-learning/crash-course/glossary#softmax) function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tRwHZmTNTX2"
      },
      "source": [
        "tf.nn.softmax(predictions[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRZmchElo481"
      },
      "source": [
        "Taking the `tf.argmax` across classes gives us the predicted class index. But, the model hasn't been trained yet, so these aren't good predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jzm_GoErz8B"
      },
      "source": [
        "print(\"Prediction: {}\".format(tf.argmax(predictions, axis=1)))\n",
        "print(\"    Labels: {}\".format(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzq2E5J2QMtw"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "*[Training](https://developers.google.com/machine-learning/crash-course/glossary#training)* is the stage of machine learning when the model is gradually optimized, or the model *learns* the dataset. The goal is to learn enough about the structure of the training dataset to make predictions about unseen data. If you learn *too much* about the training dataset, then the predictions only work for the data it has seen and will not be generalizable. This problem is called *[overfitting](https://developers.google.com/machine-learning/crash-course/glossary#overfitting)*‚Äîit's like memorizing the answers instead of understanding how to solve a problem.\n",
        "\n",
        "The Iris classification problem is an example of *[supervised machine learning](https://developers.google.com/machine-learning/glossary/#supervised_machine_learning)*: the model is trained from examples that contain labels. In *[unsupervised machine learning](https://developers.google.com/machine-learning/glossary/#unsupervised_machine_learning)*, the examples don't contain labels. Instead, the model typically finds patterns among the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaKp8aEjKX6B"
      },
      "source": [
        "### Define the loss and gradient function\n",
        "\n",
        "Both training and evaluation stages need to calculate the model's *[loss](https://developers.google.com/machine-learning/crash-course/glossary#loss)*. This measures how off a model's predictions are from the desired label, in other words, how bad the model is performing. We want to minimize, or optimize, this value.\n",
        "\n",
        "Our model will calculate its loss using the `tf.keras.losses.SparseCategoricalCrossentropy` function which takes the model's class probability predictions and the desired label, and returns the average loss across the examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOsi6b-1CXIn"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMAT4DcMPwI-"
      },
      "source": [
        "def loss(model, x, y, training):\n",
        "  # training=training is needed only if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  y_ = model(x, training=training)\n",
        "\n",
        "  return loss_object(y_true=y, y_pred=y_)\n",
        "\n",
        "\n",
        "l = loss(model, features, labels, training=False)\n",
        "print(\"Loss test: {}\".format(l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IcPqA24QM6B"
      },
      "source": [
        "Use the `tf.GradientTape` context to calculate the *[gradients](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* used to optimize your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x57HcKWhKkei"
      },
      "source": [
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets, training=True)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOxFimtlKruu"
      },
      "source": [
        "### Create an optimizer\n",
        "\n",
        "An *[optimizer](https://developers.google.com/machine-learning/crash-course/glossary#optimizer)* applies the computed gradients to the model's variables to minimize the `loss` function. You can think of the loss function as a curved surface (see Figure 3) and we want to find its lowest point by walking around. The gradients point in the direction of steepest ascent‚Äîso we'll travel the opposite way and move down the hill. By iteratively calculating the loss and gradient for each batch, we'll adjust the model during training. Gradually, the model will find the best combination of weights and bias to minimize loss. And the lower the loss, the better the model's predictions.\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://cs231n.github.io/assets/nn3/opt1.gif\" width=\"70%\"\n",
        "         alt=\"Optimization algorithms visualized over time in 3D space.\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 3.</b> Optimization algorithms visualized over time in 3D space.<br/>(Source: <a href=\"http://cs231n.github.io/neural-networks-3/\">Stanford class CS231n</a>, MIT License, Image credit: <a href=\"https://twitter.com/alecrad\">Alec Radford</a>)\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "TensorFlow has many optimization algorithms available for training. This model uses the `tf.keras.optimizers.SGD` that implements the *[stochastic gradient descent](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)* (SGD) algorithm. The `learning_rate` sets the step size to take for each iteration down the hill. This is a *hyperparameter* that you'll commonly adjust to achieve better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkUd6UiZa_dF"
      },
      "source": [
        "Let's setup the optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xxi2NNGKwG_"
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJVRZ0hP52ZB"
      },
      "source": [
        "We'll use this to calculate a single optimization step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxRNTFVe56RG"
      },
      "source": [
        "loss_value, grads = grad(model, features, labels)\n",
        "\n",
        "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
        "                                          loss_value.numpy()))\n",
        "\n",
        "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n",
        "                                          loss(model, features, labels, training=True).numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y2VSELvwAvW"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "With all the pieces in place, the model is ready for training! A training loop feeds the dataset examples into the model to help it make better predictions. The following code block sets up these training steps:\n",
        "\n",
        "1. Iterate each *epoch*. An epoch is one pass through the dataset.\n",
        "2. Within an epoch, iterate over each example in the training `Dataset` grabbing its *features* (`x`) and *label* (`y`).\n",
        "3. Using the example's features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\n",
        "4. Use an `optimizer` to update the model's variables.\n",
        "5. Keep track of some stats for visualization.\n",
        "6. Repeat for each epoch.\n",
        "\n",
        "The `num_epochs` variable is the number of times to loop over the dataset collection. Counter-intuitively, training a model longer does not guarantee a better model. `num_epochs` is a *[hyperparameter](https://developers.google.com/machine-learning/glossary/#hyperparameter)* that you can tune. Choosing the right number usually requires both experience and experimentation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIgulGRUhpto"
      },
      "source": [
        "## Note: Rerunning this cell uses the same model variables\n",
        "\n",
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 201\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "  # Training loop - using batches of 32\n",
        "  for x, y in train_dataset:\n",
        "    # Optimize the model\n",
        "    loss_value, grads = grad(model, x, y)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Track progress\n",
        "    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
        "    # Compare predicted label to actual label\n",
        "    # training=True is needed only if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    epoch_accuracy.update_state(y, model(x, training=True))\n",
        "\n",
        "  # End epoch\n",
        "  train_loss_results.append(epoch_loss_avg.result())\n",
        "  train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGnCBctFXRF3"
      },
      "source": [
        "[![](https://raw.githubusercontent.com/BackProp-fr/meetup/master/images/LogoBackPropTranspSmall.png)](https://www.backprop.fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jRzm-HyW_s7"
      },
      "source": [
        "### <font color=\"green\">Avec TensorBoard</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFVfCziGYOIq"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3V_i7NTYTeJ"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zn7E3OSZD1O"
      },
      "source": [
        "Set up summary writers to write the summaries to disk in a different logs directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV0TVsr9ZCPX"
      },
      "source": [
        "import datetime\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDbe-vLubUU-"
      },
      "source": [
        "# Define our metrics\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usfd2KKucnaE"
      },
      "source": [
        "## Note: Rerunning this cell uses the same model variables\n",
        "\n",
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 201\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # Training loop - using batches of 32\n",
        "  for x, y in train_dataset:\n",
        "    # Optimize the model\n",
        "    loss_value, grads = grad(model, x, y)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Track progress\n",
        "    train_loss.update_state(loss_value)  # Add current batch loss\n",
        "    # Compare predicted label to actual label\n",
        "    # training=True is needed only if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    train_accuracy.update_state(y, model(x, training=True))\n",
        "\n",
        "  # End epoch\n",
        "  train_loss_results.append(train_loss.result())\n",
        "  train_accuracy_results.append(train_accuracy.result())\n",
        "\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                train_loss.result(),\n",
        "                                                                train_accuracy.result()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdTbR5_dbuua"
      },
      "source": [
        "Define the training and test functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FQHVUnm_rjw"
      },
      "source": [
        "### Visualize the loss function over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3wdbmtLVTyr"
      },
      "source": [
        "While it's helpful to print out the model's training progress, it's often *more* helpful to see this progress. [TensorBoard](https://www.tensorflow.org/tensorboard) is a nice visualization tool that is packaged with TensorFlow, but we can create basic charts using the `matplotlib` module.\n",
        "\n",
        "Interpreting these charts takes some experience, but you really want to see the *loss* go down and the *accuracy* go up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agjvNd2iUGFn"
      },
      "source": [
        "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "fig.suptitle('Training Metrics')\n",
        "\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "axes[0].plot(train_loss_results)\n",
        "\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[1].plot(train_accuracy_results)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg8GoMZhLpGH"
      },
      "source": [
        "## Evaluate the model's effectiveness\n",
        "\n",
        "Now that the model is trained, we can get some statistics on its performance.\n",
        "\n",
        "*Evaluating* means determining how effectively the model makes predictions. To determine the model's effectiveness at Iris classification, pass some sepal and petal measurements to the model and ask the model to predict what Iris species they represent. Then compare the model's predictions against the actual label.  For example, a model that picked the correct species on half the input examples has an *[accuracy](https://developers.google.com/machine-learning/glossary/#accuracy)* of `0.5`. Figure 4 shows a slightly more effective model, getting 4 out of 5 predictions correct at 80% accuracy:\n",
        "\n",
        "<table cellpadding=\"8\" border=\"0\">\n",
        "  <colgroup>\n",
        "    <col span=\"4\" >\n",
        "    <col span=\"1\" bgcolor=\"lightblue\">\n",
        "    <col span=\"1\" bgcolor=\"lightgreen\">\n",
        "  </colgroup>\n",
        "  <tr bgcolor=\"lightgray\">\n",
        "    <th colspan=\"4\">Example features</th>\n",
        "    <th colspan=\"1\">Label</th>\n",
        "    <th colspan=\"1\" >Model prediction</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5.9</td><td>3.0</td><td>4.3</td><td>1.5</td><td align=\"center\">1</td><td align=\"center\">1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>6.9</td><td>3.1</td><td>5.4</td><td>2.1</td><td align=\"center\">2</td><td align=\"center\">2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td align=\"center\">0</td><td align=\"center\">0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>6.0</td> <td>3.4</td> <td>4.5</td> <td>1.6</td> <td align=\"center\">1</td><td align=\"center\" bgcolor=\"red\">2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5.5</td><td>2.5</td><td>4.0</td><td>1.3</td><td align=\"center\">1</td><td align=\"center\">1</td>\n",
        "  </tr>\n",
        "  <tr><td align=\"center\" colspan=\"6\">\n",
        "    <b>Figure 4.</b> An Iris classifier that is 80% accurate.<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-EvK7hGL0d8"
      },
      "source": [
        "### Setup the test dataset\n",
        "\n",
        "Evaluating the model is similar to training the model. The biggest difference is the examples come from a separate *[test set](https://developers.google.com/machine-learning/crash-course/glossary#test_set)* rather than the training set. To fairly assess a model's effectiveness, the examples used to evaluate a model must be different from the examples used to train the model.\n",
        "\n",
        "The setup for the test `Dataset` is similar to the setup for training `Dataset`. Download the CSV text file and parse that values, then give it a little shuffle:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps3_9dJ3Lodk"
      },
      "source": [
        "test_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
        "test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\n",
        "                                  origin=test_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRMWCu30bnxH"
      },
      "source": [
        "test_dataset = tf.data.experimental.make_csv_dataset(\n",
        "    test_fp,\n",
        "    batch_size,\n",
        "    column_names=column_names,\n",
        "    label_name='species',\n",
        "    num_epochs=1,\n",
        "    shuffle=False)\n",
        "\n",
        "test_dataset = test_dataset.map(pack_features_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFuOKXJdMAdm"
      },
      "source": [
        "### Evaluate the model on the test dataset\n",
        "\n",
        "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/#epoch) of the test data. In the following code cell, we iterate over each example in the test set and compare the model's prediction against the actual label. This is used to measure the model's accuracy across the entire test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw03-MK1cYId"
      },
      "source": [
        "test_accuracy = tf.keras.metrics.Accuracy()\n",
        "\n",
        "for (x, y) in test_dataset:\n",
        "  # training=False is needed only if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  logits = model(x, training=False)\n",
        "  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "  test_accuracy(prediction, y)\n",
        "\n",
        "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcKEZMtCOeK-"
      },
      "source": [
        "We can see on the last batch, for example, the model is usually correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNwt2eMeOane"
      },
      "source": [
        "tf.stack([y,prediction],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Li2r1tYvW7S"
      },
      "source": [
        "## Use the trained model to make predictions\n",
        "\n",
        "We've trained a model and \"proven\" that it's good‚Äîbut not perfect‚Äîat classifying Iris species. Now let's use the trained model to make some predictions on [unlabeled examples](https://developers.google.com/machine-learning/glossary/#unlabeled_example); that is, on examples that contain features but not a label.\n",
        "\n",
        "In real-life, the unlabeled examples could come from lots of different sources including apps, CSV files, and data feeds. For now, we're going to manually provide three unlabeled examples to predict their labels. Recall, the label numbers are mapped to a named representation as:\n",
        "\n",
        "* `0`: Iris setosa\n",
        "* `1`: Iris versicolor\n",
        "* `2`: Iris virginica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kesTS5Lzv-M2"
      },
      "source": [
        "predict_dataset = tf.convert_to_tensor([\n",
        "    [5.1, 3.3, 1.7, 0.5,],\n",
        "    [5.9, 3.0, 4.2, 1.5,],\n",
        "    [6.9, 3.1, 5.4, 2.1]\n",
        "])\n",
        "\n",
        "# training=False is needed only if there are layers with different\n",
        "# behavior during training versus inference (e.g. Dropout).\n",
        "predictions = model(predict_dataset, training=False)\n",
        "\n",
        "for i, logits in enumerate(predictions):\n",
        "  class_idx = tf.argmax(logits).numpy()\n",
        "  p = tf.nn.softmax(logits)[class_idx]\n",
        "  name = class_names[class_idx]\n",
        "  print(\"Example {} prediction: {} ({:4.1f}%)\".format(i, name, 100*p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQLA40CD-SgQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}